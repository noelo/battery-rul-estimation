{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jw9FMur02UtZ"
   },
   "source": [
    "# Autoencoder UNIBO Powertools Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XKxZ90kO2Uta"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 11:54:35.052306: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-10 11:54:35.192692: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-10 11:54:35.192708: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-10 11:54:36.017208: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-10 11:54:36.017281: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-10 11:54:36.017288: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import math\n",
    "import os\n",
    "import ntpath\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from importlib import reload\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "IS_COLAB = False\n",
    "IS_TRAINING = True\n",
    "RESULT_NAME = \"\"\n",
    "\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    data_path = \"/content/drive/My Drive/battery-state-estimation/battery-state-estimation/\"\n",
    "else:\n",
    "    data_path = \"../../\"\n",
    "\n",
    "sys.path.append(data_path)\n",
    "from data_processing.unibo_powertools_data import UniboPowertoolsData, CycleCols\n",
    "from data_processing.model_data_handler import ModelDataHandler\n",
    "from data_processing.prepare_rul_data import RulHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfVCRISs2Utc"
   },
   "source": [
    "### Config logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "K2IvySBk2Utd"
   },
   "outputs": [],
   "source": [
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s [%(levelname)s]: %(message)s', level=logging.DEBUG, datefmt='%Y/%m/%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsbkwTX22Utf"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DrHYRy-a2Utg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/10 11:54:44 [DEBUG]: Start loading data with lines: [37, 40], types: [] and chunksize: 1000000...\n",
      "2023/03/10 11:55:11 [DEBUG]: Finish loading data.\n",
      "2023/03/10 11:55:11 [INFO]: Loaded raw dataset A data with cycle row count: 15384908 and capacity row count: 39585\n",
      "2023/03/10 11:55:11 [DEBUG]: Start cleaning cycle raw data...\n",
      "2023/03/10 11:55:19 [DEBUG]: Finish cleaning cycle raw data.\n",
      "2023/03/10 11:55:19 [INFO]: Removed 11 rows of abnormal cycle raw data.\n",
      "2023/03/10 11:55:19 [DEBUG]: Start cleaning capacity raw data...\n",
      "2023/03/10 11:55:19 [DEBUG]: Finish cleaning capacity raw data.\n",
      "2023/03/10 11:55:19 [INFO]: Removed 1 rows of abnormal capacity raw data.\n",
      "2023/03/10 11:55:19 [DEBUG]: Start assigning charging raw data...\n",
      "2023/03/10 11:55:19 [DEBUG]: Finish assigning charging raw data.\n",
      "2023/03/10 11:55:19 [INFO]: [Charging] cycle raw count: 12160812, capacity raw count: 19800\n",
      "2023/03/10 11:55:19 [DEBUG]: Start assigning discharging raw data...\n",
      "2023/03/10 11:55:20 [DEBUG]: Finish assigning discharging raw data.\n",
      "2023/03/10 11:55:20 [INFO]: [Discharging] cycle raw count: 3224085, capacity raw count: 19784\n"
     ]
    }
   ],
   "source": [
    "dataset = UniboPowertoolsData(\n",
    "    test_types=[],\n",
    "    chunk_size=1000000,\n",
    "    lines=[37, 40],\n",
    "    charge_line=37,\n",
    "    discharge_line=40,\n",
    "    base_path=data_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NSFp-2Rl2Utj"
   },
   "outputs": [],
   "source": [
    "train_names = [\n",
    "    '000-DM-3.0-4019-S',#minimum capacity 1.48\n",
    "    '001-DM-3.0-4019-S',#minimum capacity 1.81\n",
    "    '002-DM-3.0-4019-S',#minimum capacity 2.06\n",
    "\n",
    "    '009-DM-3.0-4019-H',#minimum capacity 1.41\n",
    "    '010-DM-3.0-4019-H',#minimum capacity 1.44\n",
    "\n",
    "    '014-DM-3.0-4019-P',#minimum capacity 1.7\n",
    "    '015-DM-3.0-4019-P',#minimum capacity 1.76\n",
    "    '016-DM-3.0-4019-P',#minimum capacity 1.56\n",
    "    '017-DM-3.0-4019-P',#minimum capacity 1.29\n",
    "    #'047-DM-3.0-4019-P',#new 1.98\n",
    "    #'049-DM-3.0-4019-P',#new 2.19\n",
    "\n",
    "\n",
    "\n",
    "    '007-EE-2.85-0820-S',#2.5\n",
    "    '008-EE-2.85-0820-S',#2.49\n",
    "    '042-EE-2.85-0820-S',#2.51\n",
    "\n",
    "    '043-EE-2.85-0820-H',#2.31\n",
    "\n",
    "\n",
    "    '040-DM-4.00-2320-S',#minimum capacity 3.75, cycles 188\n",
    "\n",
    "\n",
    "    '018-DP-2.00-1320-S',#minimum capacity 1.82\n",
    "    #'019-DP-2.00-1320-S',#minimum capacity 1.61\n",
    "    '036-DP-2.00-1720-S',#minimum capacity 1.91\n",
    "    '037-DP-2.00-1720-S',#minimum capacity 1.84\n",
    "    '038-DP-2.00-2420-S',#minimum capacity 1.854 (to 0)\n",
    "    '050-DP-2.00-4020-S',#new 1.81\n",
    "    '051-DP-2.00-4020-S',#new 1.866    \n",
    "    \n",
    "]\n",
    "\n",
    "test_names = [\n",
    "    '003-DM-3.0-4019-S',#minimum capacity 1.84\n",
    "\n",
    "    '011-DM-3.0-4019-H',#minimum capacity 1.36\n",
    "\n",
    "    '013-DM-3.0-4019-P',#minimum capacity 1.6\n",
    "\n",
    "\n",
    "\n",
    "    '006-EE-2.85-0820-S',# 2.621\n",
    "    \n",
    "    '044-EE-2.85-0820-H',# 2.43\n",
    "\n",
    "\n",
    "\n",
    "    '039-DP-2.00-2420-S',#minimum capacity 1.93\n",
    "\n",
    "\n",
    "\n",
    "    '041-DM-4.00-2320-S',#minimum capacity 3.76, cycles 190\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "k-yTrXQ12Utm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/10 11:58:02 [DEBUG]: Start preparing data for training: ['000-DM-3.0-4019-S', '001-DM-3.0-4019-S', '002-DM-3.0-4019-S', '009-DM-3.0-4019-H', '010-DM-3.0-4019-H', '014-DM-3.0-4019-P', '015-DM-3.0-4019-P', '016-DM-3.0-4019-P', '017-DM-3.0-4019-P', '007-EE-2.85-0820-S', '008-EE-2.85-0820-S', '042-EE-2.85-0820-S', '043-EE-2.85-0820-H', '040-DM-4.00-2320-S', '018-DP-2.00-1320-S', '036-DP-2.00-1720-S', '037-DP-2.00-1720-S', '038-DP-2.00-2420-S', '050-DP-2.00-4020-S', '051-DP-2.00-4020-S'] and testing: ['003-DM-3.0-4019-S', '011-DM-3.0-4019-H', '013-DM-3.0-4019-P', '006-EE-2.85-0820-S', '044-EE-2.85-0820-H', '039-DP-2.00-2420-S', '041-DM-4.00-2320-S']...\n",
      "2023/03/10 11:58:18 [DEBUG]: Finish getting training and testing charge data.\n",
      "2023/03/10 11:58:27 [DEBUG]: Finish getting training and testing discharge data.\n",
      "2023/03/10 11:58:27 [DEBUG]: Finish cleaning training and testing charge data.\n",
      "2023/03/10 11:58:27 [DEBUG]: Finish cleaning training and testing discharge data.\n",
      "2023/03/10 11:58:28 [DEBUG]: Finish adding training and testing discharge SOC parameters.\n",
      "2023/03/10 11:58:33 [DEBUG]: Finish adding training and testing discharge SOH parameters.\n",
      "2023/03/10 11:58:33 [DEBUG]: Finish preparing data.\n",
      "2023/03/10 11:58:33 [INFO]: Prepared training charge cycle data: (11325,), capacity data: (11325, 15)\n",
      "2023/03/10 11:58:33 [INFO]: Prepared testing charge cycle data: (3507,), capacity data: (3507, 15)\n",
      "2023/03/10 11:58:33 [INFO]: Prepared training discharge cycle data: (11325,), capacity data: (11325, 20)\n",
      "2023/03/10 11:58:33 [INFO]: Prepared testing discharge cycle data: (3507,), capacity data: (3507, 20)\n"
     ]
    }
   ],
   "source": [
    "dataset.prepare_data(train_names, test_names)\n",
    "dataset_handler = ModelDataHandler(dataset, [\n",
    "    CycleCols.VOLTAGE,\n",
    "    CycleCols.CURRENT,\n",
    "    CycleCols.TEMPERATURE\n",
    "])\n",
    "\n",
    "rul_handler = RulHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAAxueIqkZM9"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TbntMzBZkZM9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../data_processing/model_data_handler.py:92: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  list(map(lambda data: data[:, x_indices].astype('float32'), cyc))\n",
      "2023/03/10 11:58:50 [INFO]: Train x: (11325, 287, 3), train y soh: (11325, 1) | Test x: (3507, 287, 3), test y soh: (3507, 1) | \n",
      "                            battery n cycle train: (20,), battery n cycle test: (7,), \n",
      "                            time train: (11325, 287, 1), time test: (3507, 287, 1) |\n",
      "                            raw current train: (11325, 287), raw current test: (3507, 287) |\n",
      "                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut train shape (11325, 284, 3)\n",
      "cut test shape (3507, 284, 3)\n"
     ]
    }
   ],
   "source": [
    "(train_x, train_y_soh, test_x, test_y_soh,\n",
    "  train_battery_range, test_battery_range,\n",
    "  time_train, time_test, current_train, current_test) = dataset_handler.get_discharge_whole_cycle_future(train_names, test_names)\n",
    "\n",
    "train_x = train_x[:,:284,:]\n",
    "test_x = test_x[:,:284,:]\n",
    "print(\"cut train shape {}\".format(train_x.shape))\n",
    "print(\"cut test shape {}\".format(test_x.shape))\n",
    "\n",
    "\n",
    "x_norm = rul_handler.Normalization()\n",
    "train_x, test_x = x_norm.fit_and_normalize(train_x, test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iYU-n0K2Utq"
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LSx96n4w2Uts"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-10-11-59-00_autoencoder_unibo_powertools\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 11:59:00.714480: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-10 11:59:00.714747: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-10 11:59:00.714777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (nocworkp1): /proc/driver/nvidia/version does not exist\n",
      "2023-03-10 11:59:00.718112: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 142, 16)           256       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 71, 8)             392       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 568)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5690      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,338\n",
      "Trainable params: 6,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 568)               6248      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 71, 8)             0         \n",
      "                                                                 \n",
      " conv1d_transpose (Conv1DTra  (None, 142, 8)           200       \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv1d_transpose_1 (Conv1DT  (None, 284, 16)          656       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 284, 3)            147       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,251\n",
      "Trainable params: 7,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if IS_TRAINING:\n",
    "    EXPERIMENT = \"autoencoder_unibo_powertools\"\n",
    "\n",
    "    experiment_name = time.strftime(\"%Y-%m-%d-%H-%M-%S\") + '_' + EXPERIMENT\n",
    "    print(experiment_name)\n",
    "\n",
    "# Model definition\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "LATENT_DIM = 10\n",
    "\n",
    "class Autoencoder(Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=(train_x.shape[1], train_x.shape[2])),\n",
    "            #layers.MaxPooling1D(5, padding='same'),\n",
    "            layers.Conv1D(filters=16, kernel_size=5, strides=2, activation='relu', padding='same'),\n",
    "            layers.Conv1D(filters=8, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(self.latent_dim, activation='relu')\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=(self.latent_dim)),\n",
    "            layers.Dense(568, activation='relu'),\n",
    "            layers.Reshape((71, 8)),\n",
    "            layers.Conv1DTranspose(filters=8, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "            layers.Conv1DTranspose(filters=16, kernel_size=5, strides=2, activation='relu', padding='same'),\n",
    "            layers.Conv1D(3, kernel_size=3, activation='relu', padding='same'),\n",
    "            #layers.UpSampling1D(5),\n",
    "        ])\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "autoencoder = Autoencoder(LATENT_DIM)\n",
    "autoencoder.compile(optimizer=opt, loss='mse', metrics=['mse', 'mae', 'mape', tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "autoencoder.encoder.summary()\n",
    "autoencoder.decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AIEcv6Ey2Utu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "319/319 [==============================] - 2s 6ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0155 - mape: 587093.5625 - rmse: 0.0317 - val_loss: 7.7785e-04 - val_mse: 7.7785e-04 - val_mae: 0.0142 - val_mape: 692674.8125 - val_rmse: 0.0279\n",
      "Epoch 2/5\n",
      "319/319 [==============================] - 2s 6ms/step - loss: 9.3752e-04 - mse: 9.3752e-04 - mae: 0.0151 - mape: 532729.6250 - rmse: 0.0306 - val_loss: 8.2071e-04 - val_mse: 8.2071e-04 - val_mae: 0.0155 - val_mape: 636668.6250 - val_rmse: 0.0286\n",
      "Epoch 3/5\n",
      "319/319 [==============================] - 2s 5ms/step - loss: 8.7400e-04 - mse: 8.7400e-04 - mae: 0.0148 - mape: 484488.0625 - rmse: 0.0296 - val_loss: 7.6117e-04 - val_mse: 7.6117e-04 - val_mae: 0.0151 - val_mape: 546387.0625 - val_rmse: 0.0276\n",
      "Epoch 4/5\n",
      "319/319 [==============================] - 2s 5ms/step - loss: 8.1554e-04 - mse: 8.1554e-04 - mae: 0.0145 - mape: 433230.0000 - rmse: 0.0286 - val_loss: 6.6900e-04 - val_mse: 6.6900e-04 - val_mae: 0.0143 - val_mape: 509022.7188 - val_rmse: 0.0259\n",
      "Epoch 5/5\n",
      "319/319 [==============================] - 2s 5ms/step - loss: 7.5798e-04 - mse: 7.5798e-04 - mae: 0.0141 - mape: 376776.9062 - rmse: 0.0275 - val_loss: 6.9525e-04 - val_mse: 6.9525e-04 - val_mae: 0.0151 - val_mape: 394587.7500 - val_rmse: 0.0264\n"
     ]
    }
   ],
   "source": [
    "if IS_TRAINING:\n",
    "    history = autoencoder.fit(train_x, train_x,\n",
    "                                epochs=5, \n",
    "                                batch_size=32, \n",
    "                                verbose=1,\n",
    "                                validation_split=0.1\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "oNHlqcvP2Utx"
   },
   "outputs": [],
   "source": [
    "if IS_TRAINING:\n",
    "    from zipfile import ZipFile\n",
    "    zipObj = ZipFile('sample.zip', 'w')\n",
    "    autoencoder.save_weights(data_path + 'results/trained_model/%s/modelNOC' % experiment_name,\"h5\")\n",
    "\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    hist_csv_file = data_path + 'results/trained_model/%s/history.csvn' % experiment_name\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "    history = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_TRAINING:\n",
    "    history = pd.read_csv(data_path + 'results/trained_model/%s/history.csv' % RESULT_NAME)\n",
    "    autoencoder.load_weights(data_path + 'results/trained_model/%s/model' % RESULT_NAME)\n",
    "    autoencoder.encoder.summary()\n",
    "    autoencoder.decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_TRAINING:\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LH5RANQIEQVx"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggNKW-VqENFN"
   },
   "outputs": [],
   "source": [
    "results = autoencoder.evaluate(test_x, test_x, return_dict = True)\n",
    "print(results)\n",
    "max_rmse = 0\n",
    "for index in range(test_x.shape[0]):\n",
    "    result = autoencoder.evaluate(np.array([test_x[index, :, :]]), np.array([test_x[index, :, :]]), return_dict = True, verbose=0)\n",
    "    max_rmse = max(max_rmse, result['rmse'])\n",
    "print(\"Max rmse: {}\".format(max_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiqyD8Bn2Utz"
   },
   "source": [
    "# Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jH9RrBRN2Utz"
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=history['loss'],\n",
    "                    mode='lines', name='train'))\n",
    "if 'val_loss' in history:\n",
    "    fig.add_trace(go.Scatter(y=history['val_loss'],\n",
    "                    mode='lines', name='validation'))\n",
    "fig.update_layout(title='Loss trend',\n",
    "                  xaxis_title='epoch',\n",
    "                  yaxis_title='loss',\n",
    "                  width=1400,\n",
    "                  height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = autoencoder.predict(train_x)\n",
    "labels = ['Voltage', 'Current', 'Temperature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_x.shape[2]):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(y=train_predictions[0,:,i],\n",
    "                        mode='lines', name='predicted'))\n",
    "    fig.add_trace(go.Scatter(y=train_x[0,:,i],\n",
    "                        mode='lines', name='actual'))\n",
    "    fig.update_layout(title='Results on training - battery new',\n",
    "                    xaxis_title='Step',\n",
    "                    yaxis_title=labels[i],\n",
    "                    width=1400,\n",
    "                    height=600)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_x.shape[2]):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(y=train_predictions[int(train_battery_range[0]/2),:,i],\n",
    "                        mode='lines', name='predicted'))\n",
    "    fig.add_trace(go.Scatter(y=train_x[int(train_battery_range[0]/2),:,i],\n",
    "                        mode='lines', name='actual'))\n",
    "    fig.update_layout(title='Results on training - middle life',\n",
    "                    xaxis_title='Step',\n",
    "                    yaxis_title=labels[i],\n",
    "                    width=1400,\n",
    "                    height=600)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_x.shape[2]):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(y=train_predictions[train_battery_range[0]-1,:,i],\n",
    "                        mode='lines', name='predicted'))\n",
    "    fig.add_trace(go.Scatter(y=train_x[train_battery_range[0]-1,:,i],\n",
    "                        mode='lines', name='actual'))\n",
    "    fig.update_layout(title='Results on training - end of life',\n",
    "                    xaxis_title='Step',\n",
    "                    yaxis_title=labels[i],\n",
    "                    width=1400,\n",
    "                    height=600)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = autoencoder.predict(test_x)\n",
    "labels = ['Voltage', 'Current', 'Temperature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_x.shape[2]):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(y=test_predictions[0,:,i],\n",
    "                        mode='lines', name='predicted'))\n",
    "    fig.add_trace(go.Scatter(y=test_x[0,:,i],\n",
    "                        mode='lines', name='actual'))\n",
    "    fig.update_layout(title='Results on testing - battery new',\n",
    "                    xaxis_title='Step',\n",
    "                    yaxis_title=labels[i],\n",
    "                    width=1400,\n",
    "                    height=600)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_x.shape[2]):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(y=test_predictions[int(test_battery_range[0]/2),:,i],\n",
    "                        mode='lines', name='predicted'))\n",
    "    fig.add_trace(go.Scatter(y=test_x[0,:,i],\n",
    "                        mode='lines', name='actual'))\n",
    "    fig.update_layout(title='Results on testing - middle life',\n",
    "                    xaxis_title='Step',\n",
    "                    yaxis_title=labels[i],\n",
    "                    width=1400,\n",
    "                    height=600)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_x.shape[2]):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(y=test_predictions[test_battery_range[0]-1,:,i],\n",
    "                        mode='lines', name='predicted'))\n",
    "    fig.add_trace(go.Scatter(y=test_x[0,:,i],\n",
    "                        mode='lines', name='actual'))\n",
    "    fig.update_layout(title='Results on testing - end of life',\n",
    "                    xaxis_title='Step',\n",
    "                    yaxis_title=labels[i],\n",
    "                    width=1400,\n",
    "                    height=600)\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_soh_prediction_only_future.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
