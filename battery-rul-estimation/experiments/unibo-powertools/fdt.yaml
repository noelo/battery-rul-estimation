apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: loadfilestest
  annotations:
    tekton.dev/output_artifacts: '{"fitnormalisedata": [{"key": "artifacts/$PIPELINERUN/fitnormalisedata/modelData.tgz",
      "name": "fitnormalisedata-modelData", "path": "/tmp/outputs/modelData/data"}],
      "readydata": [{"key": "artifacts/$PIPELINERUN/readydata/preppedData.tgz", "name":
      "readydata-preppedData", "path": "/tmp/outputs/preppedData/data"}]}'
    tekton.dev/input_artifacts: '{"autoencodedata": [{"name": "fitnormalisedata-modelData",
      "parent_task": "fitnormalisedata"}], "fitnormalisedata": [{"name": "readydata-preppedData",
      "parent_task": "readydata"}]}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"autoencodedata": [], "fitnormalisedata": [["modelData",
      "$(workspaces.fitnormalisedata.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/modelData"]],
      "readydata": [["preppedData", "$(workspaces.readydata.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/preppedData"]]}'
    sidecar.istio.io/inject: "false"
    tekton.dev/template: ''
    pipelines.kubeflow.org/big_data_passing_format: $(workspaces.$TASK_NAME.path)/artifacts/$ORIG_PR_NAME/$TASKRUN_NAME/$TASK_PARAM_NAME
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Download files from minio
      and store", "name": "loadFilesTest"}'
  labels:
    pipelines.kubeflow.org/pipelinename: ''
    pipelines.kubeflow.org/generation: ''
spec:
  pipelineSpec:
    tasks:
    - name: readydata
      taskSpec:
        steps:
        - name: main
          args:
          - --preppedData
          - $(workspaces.readydata.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/preppedData
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'boto3' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n\
            \    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return\
            \ file_path\n\ndef readyData(preppedData):\n    import boto3\n    import\
            \ os\n    import sys\n    import logging\n    import pickle\n    from\
            \ importlib import reload\n    import shutil\n\n# export DEFAULT_ACCESSMODES=ReadWriteOnce\n\
            # export DEFAULT_STORAGE_SIZE=5Gi\n# export DEFAULT_STORAGE_CLASS=odf-lvm-vg1\n\
            \n    reload(logging)\n    logging.basicConfig(format='%(asctime)s [%(levelname)s]:\
            \ %(message)s', level=logging.DEBUG, datefmt='%Y/%m/%d %H:%M:%S')    \n\
            \n    from data_processing.unibo_powertools_data import UniboPowertoolsData,\
            \ CycleCols\n    from data_processing.model_data_handler import ModelDataHandler\n\
            \    from data_processing.prepare_rul_data import RulHandler\n\n    data_path\
            \ = \"/mnt/\"    \n    sys.path.append(data_path)\n    print(sys.path)\n\
            \n    dataset = UniboPowertoolsData(\n        test_types=[],\n       \
            \ chunk_size=1000000,\n        lines=[37, 40],\n        charge_line=37,\n\
            \        discharge_line=40,\n        base_path=data_path\n    )\n\n  \
            \  with open(preppedData, \"b+w\") as f:   \n        pickle.dump(dataset,f)\
            \    \n    print('PrepData written...')\n\nimport argparse\n_parser =\
            \ argparse.ArgumentParser(prog='ReadyData', description='')\n_parser.add_argument(\"\
            --preppedData\", dest=\"preppedData\", type=_make_parent_dirs_and_return_path,\
            \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
            \n_outputs = readyData(**_parsed_args)\n"
          image: quay.io/noeloc/batterybase
          volumeMounts:
          - mountPath: /mnt
            name: batterydatavol
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        - image: busybox
          name: output-taskrun-name
          command:
          - sh
          - -ec
          - echo -n "$(context.taskRun.name)" > "$(results.taskrun-name.path)"
        - image: busybox
          name: copy-results-artifacts
          command:
          - sh
          - -ec
          - |
            set -exo pipefail
            TOTAL_SIZE=0
            copy_artifact() {
            if [ -d "$1" ]; then
              tar -czvf "$1".tar.gz "$1"
              SUFFIX=".tar.gz"
            fi
            ARTIFACT_SIZE=`wc -c "$1"${SUFFIX} | awk '{print $1}'`
            TOTAL_SIZE=$( expr $TOTAL_SIZE + $ARTIFACT_SIZE)
            touch "$2"
            if [[ $TOTAL_SIZE -lt 3072 ]]; then
              if [ -d "$1" ]; then
                tar -tzf "$1".tar.gz > "$2"
              elif ! awk "/[^[:print:]]/{f=1} END{exit !f}" "$1"; then
                cp "$1" "$2"
              fi
            fi
            }
            copy_artifact $(workspaces.readydata.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/preppedData $(results.preppedData.path)
          onError: continue
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        results:
        - name: preppedData
          type: string
          description: /tmp/outputs/preppedData/data
        - name: taskrun-name
          type: string
        volumes:
        - name: batterydatavol
          persistentVolumeClaim:
            claimName: batterydata
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "ReadyData", "outputs":
              [{"name": "preppedData"}], "version": "ReadyData@sha256=b291392d88e1c88099495a57b832ae397a2f08de7630234812fd3501db084e23"}'
        workspaces:
        - name: readydata
      workspaces:
      - name: readydata
        workspace: loadfilestest
    - name: fitnormalisedata
      params:
      - name: readydata-trname
        value: $(tasks.readydata.results.taskrun-name)
      taskSpec:
        steps:
        - name: main
          args:
          - --preppedData
          - $(workspaces.fitnormalisedata.path)/artifacts/$ORIG_PR_NAME/$(params.readydata-trname)/preppedData
          - --modelData
          - $(workspaces.fitnormalisedata.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/modelData
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'boto3' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n\
            \    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return\
            \ file_path\n\ndef fitNormaliseData(preppedData,modelData):\n    import\
            \ boto3\n    import os\n    import sys\n    import logging\n    import\
            \ pickle\n    from importlib import reload\n    import shutil\n    reload(logging)\n\
            \    logging.basicConfig(format='%(asctime)s [%(levelname)s]: %(message)s',\
            \ level=logging.DEBUG, datefmt='%Y/%m/%d %H:%M:%S')    \n\n    from data_processing.unibo_powertools_data\
            \ import UniboPowertoolsData, CycleCols\n    from data_processing.model_data_handler\
            \ import ModelDataHandler\n    from data_processing.prepare_rul_data import\
            \ RulHandler\n\n    f = open(preppedData,\"b+r\")\n    dataset = pickle.load(f)\n\
            \n    train_names = [\n        '000-DM-3.0-4019-S',#minimum capacity 1.48\n\
            \        '001-DM-3.0-4019-S',#minimum capacity 1.81\n        '002-DM-3.0-4019-S',#minimum\
            \ capacity 2.06\n        '009-DM-3.0-4019-H',#minimum capacity 1.41\n\
            \        '010-DM-3.0-4019-H',#minimum capacity 1.44\n        '014-DM-3.0-4019-P',#minimum\
            \ capacity 1.7\n        '015-DM-3.0-4019-P',#minimum capacity 1.76\n \
            \       '016-DM-3.0-4019-P',#minimum capacity 1.56\n        '017-DM-3.0-4019-P',#minimum\
            \ capacity 1.29\n        #'047-DM-3.0-4019-P',#new 1.98\n        #'049-DM-3.0-4019-P',#new\
            \ 2.19\n        '007-EE-2.85-0820-S',#2.5\n        '008-EE-2.85-0820-S',#2.49\n\
            \        '042-EE-2.85-0820-S',#2.51\n        '043-EE-2.85-0820-H',#2.31\n\
            \        '040-DM-4.00-2320-S',#minimum capacity 3.75, cycles 188\n   \
            \     '018-DP-2.00-1320-S',#minimum capacity 1.82\n        #'019-DP-2.00-1320-S',#minimum\
            \ capacity 1.61\n        '036-DP-2.00-1720-S',#minimum capacity 1.91\n\
            \        '037-DP-2.00-1720-S',#minimum capacity 1.84\n        '038-DP-2.00-2420-S',#minimum\
            \ capacity 1.854 (to 0)\n        '050-DP-2.00-4020-S',#new 1.81\n    \
            \    '051-DP-2.00-4020-S',#new 1.866 \n    ]\n\n    test_names = [\n \
            \       '003-DM-3.0-4019-S',#minimum capacity 1.84\n        '011-DM-3.0-4019-H',#minimum\
            \ capacity 1.36\n        '013-DM-3.0-4019-P',#minimum capacity 1.6\n \
            \       '006-EE-2.85-0820-S',# 2.621    \n        '044-EE-2.85-0820-H',#\
            \ 2.43\n        '039-DP-2.00-2420-S',#minimum capacity 1.93\n        '041-DM-4.00-2320-S',#minimum\
            \ capacity 3.76, cycles 190\n    ]\n\n    # %%\n    dataset.prepare_data(train_names,\
            \ test_names)\n    dataset_handler = ModelDataHandler(dataset, [\n   \
            \     CycleCols.VOLTAGE,\n        CycleCols.CURRENT,\n        CycleCols.TEMPERATURE\n\
            \    ])\n\n    rul_handler = RulHandler()\n    # %%\n    (train_x, train_y_soh,\
            \ test_x, test_y_soh,\n    train_battery_range, test_battery_range,\n\
            \    time_train, time_test, current_train, current_test) = dataset_handler.get_discharge_whole_cycle_future(train_names,\
            \ test_names)\n\n    train_x = train_x[:,:284,:]\n    test_x = test_x[:,:284,:]\n\
            \    print(\"cut train shape {}\".format(train_x.shape))\n    print(\"\
            cut test shape {}\".format(test_x.shape))\n\n    x_norm = rul_handler.Normalization()\n\
            \    train_x, test_x = x_norm.fit_and_normalize(train_x, test_x) \n\n\
            \    print(\"cut train shape {}\".format(train_x.shape))\n    print(\"\
            cut test shape {}\".format(test_x.shape))\n\n    dataStore = [train_x,\
            \ train_y_soh, test_x, test_y_soh,train_battery_range, test_battery_range,time_train,\
            \ time_test, current_train, current_test]  \n\n    with open(modelData,\
            \ \"b+w\") as f:   \n        pickle.dump(dataStore,f)    \n    print('modelData\
            \ written...')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='FitNormaliseData',\
            \ description='')\n_parser.add_argument(\"--preppedData\", dest=\"preppedData\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --modelData\", dest=\"modelData\", type=_make_parent_dirs_and_return_path,\
            \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
            \n_outputs = fitNormaliseData(**_parsed_args)\n"
          image: quay.io/noeloc/batterybase
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        - image: busybox
          name: output-taskrun-name
          command:
          - sh
          - -ec
          - echo -n "$(context.taskRun.name)" > "$(results.taskrun-name.path)"
        - image: busybox
          name: copy-results-artifacts
          command:
          - sh
          - -ec
          - |
            set -exo pipefail
            TOTAL_SIZE=0
            copy_artifact() {
            if [ -d "$1" ]; then
              tar -czvf "$1".tar.gz "$1"
              SUFFIX=".tar.gz"
            fi
            ARTIFACT_SIZE=`wc -c "$1"${SUFFIX} | awk '{print $1}'`
            TOTAL_SIZE=$( expr $TOTAL_SIZE + $ARTIFACT_SIZE)
            touch "$2"
            if [[ $TOTAL_SIZE -lt 3072 ]]; then
              if [ -d "$1" ]; then
                tar -tzf "$1".tar.gz > "$2"
              elif ! awk "/[^[:print:]]/{f=1} END{exit !f}" "$1"; then
                cp "$1" "$2"
              fi
            fi
            }
            copy_artifact $(workspaces.fitnormalisedata.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/modelData $(results.modelData.path)
          onError: continue
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        params:
        - name: readydata-trname
        results:
        - name: modelData
          type: string
          description: /tmp/outputs/modelData/data
        - name: taskrun-name
          type: string
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "FitNormaliseData",
              "outputs": [{"name": "modelData"}], "version": "FitNormaliseData@sha256=da1b181eeccb5b2d55d5fd1955080ef88c677bf8b36a2dd03bcc5f38da9eb054"}'
        workspaces:
        - name: fitnormalisedata
      workspaces:
      - name: fitnormalisedata
        workspace: loadfilestest
      runAfter:
      - readydata
    - name: autoencodedata
      params:
      - name: fitnormalisedata-trname
        value: $(tasks.fitnormalisedata.results.taskrun-name)
      taskSpec:
        steps:
        - name: main
          args:
          - --modelData
          - $(workspaces.autoencodedata.path)/artifacts/$ORIG_PR_NAME/$(params.fitnormalisedata-trname)/modelData
          command:
          - sh
          - -c
          - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
            'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
            --no-warn-script-location 'boto3' --user) && "$0" "$@"
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def autoEncodeData(modelData):\n    import tensorflow as tf\n    from\
            \ tensorflow import keras\n    from tensorflow.keras import layers, regularizers\n\
            \    from tensorflow.keras.models import Model\n    import logging\n \
            \   import pickle\n    from importlib import reload\n    reload(logging)\n\
            \    logging.basicConfig(format='%(asctime)s [%(levelname)s]: %(message)s',\
            \ level=logging.DEBUG, datefmt='%Y/%m/%d %H:%M:%S')    \n\n    from data_processing.unibo_powertools_data\
            \ import UniboPowertoolsData, CycleCols\n    from data_processing.model_data_handler\
            \ import ModelDataHandler\n    from data_processing.prepare_rul_data import\
            \ RulHandler\n\n    f = open(modelData,\"b+r\")\n    dataStore = pickle.load(f)\n\
            \n    train_x=dataStore[0]\n    train_y_soh=dataStore[1]\n    test_x=dataStore[2]\n\
            \    test_y_soh=dataStore[3]\n    train_battery_range=dataStore[4]\n \
            \   test_battery_range=dataStore[5]\n    time_train=dataStore[6]\n   \
            \ time_test=dataStore[7]\n    current_train=dataStore[8]\n    current_test=dataStore[9]\
            \  \n\n    print(\"cut train shape {}\".format(train_x.shape))\n    print(\"\
            cut test shape {}\".format(test_x.shape))\n\nimport argparse\n_parser\
            \ = argparse.ArgumentParser(prog='AutoEncodeData', description='')\n_parser.add_argument(\"\
            --modelData\", dest=\"modelData\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parsed_args = vars(_parser.parse_args())\n\n_outputs = autoEncodeData(**_parsed_args)\n"
          image: quay.io/noeloc/batterybase
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        params:
        - name: fitnormalisedata-trname
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "AutoEncodeData",
              "outputs": [], "version": "AutoEncodeData@sha256=449163b4f02f828427e2202b5a363dbf59b5e4f828ced5998359e48c1b78dc72"}'
        workspaces:
        - name: autoencodedata
      workspaces:
      - name: autoencodedata
        workspace: loadfilestest
      runAfter:
      - fitnormalisedata
    workspaces:
    - name: loadfilestest
  workspaces:
  - name: loadfilestest
    volumeClaimTemplate:
      spec:
        storageClassName: odf-lvm-vg1
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 5Gi
